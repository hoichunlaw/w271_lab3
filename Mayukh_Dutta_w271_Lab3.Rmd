---
title: 'Statistical Methods for Discrete Response, Time Series, and Panel Data (W271): Group Lab 3'
geometry: margin=1in
output:
  pdf_document:
    latex_engine: xelatex
  number_sections: yes
  html_document: default
  toc: yes
fontsize: 11pt
---

## Instructions (Please Read Carefully):

* $\textbf{Due Date: Sunday 04/19/20 11:59pm}$

* 20 page limit 

* Do not modify fontsize, margin or line-spacing settings

* One student from each group should submit the lab to their student github repo by the deadline; submission and revisions made after the deadline will not be graded

* Answers should clearly explain your reasoning; do not simply 'output dump' the results of code without explanation 

* Submit two files:
    
    1. A pdf file that details your answers. Include all R code used to produce the answers. Do not suppress the codes in your pdf file
    
    2. The R markdown (Rmd) file used to produce the pdf file
  
    The assignment will not be graded unless **both** files are submitted
      
* Name your files to include all group members names. For example the students' names are Stan Cartman and Kenny Kyle, name your files as follows:

    * `StanCartman_KennyKyle_Lab3.Rmd`
    * `StanCartman_KennyKyle_Lab3.pdf`
            
* Although it sounds obvious, please write your names on page 1 of your pdf and Rmd files

* For statistical methods that we cover in this course, use the R libraries and functions that are covered in this course. If you use libraries and functions for statistical modeling that we have not covered, you must provide an explanation of why such libraries and functions are used and reference the library documentation. For data wrangling and data visualization, you are free to use other libraries, such as `dplyr`, `ggplot2`, etc.

* Your report needs to include:

    * A thorough analysis of the given dataset, which includ examiniation of anomalies, missing values, potential of top and/or bottom code, and other potential anomalies, in each of the variables.
        
    * A comprehensive Exploratory Data Analysis (EDA) analysis, which includes both graphical and tabular analysis, as taught in this course. Output-dump (that is, graphs and tables that don't come with explanations) will result in a very low, if not zero, score. Be selective when choosing visuals and tables to illustrate your key points and concise with your explanations (please do not ramble). 
    
    * A proper narrative for each question answered. Make sure that your audience can easily follow the logic of your analysis and the rationale of decisions made in your modeling, supported by empirical evidence. Use the insights generated from your EDA step to guide your modeling approach.
    
    * Clear explanations of all steps used to arrive at a final model, with conclusions that summarize results with respect to the question(s) being asked and key takeaways from the analysis.

* For mathematical formulae, type them in your R markdown file. Do not e.g. write them on a piece of paper, snap a photo, and use the image file.

* Incorrectly following submission instructions results in deduction of grades

* Students are expected to act with regard to UC Berkeley Academic Integrity

\newpage

# U.S. traffic fatalities: 1980-2004

In this lab, you are asked to answer the question **"Do changes in traffic laws affect traffic fatalities?"**  To do so, you will conduct the tasks specified below using the data set *driving.Rdata*, which includes 25 years of data that cover changes in various state drunk driving, seat belt, and speed limit laws. 

Specifically, this data set contains data for the 48 continental U.S. states from 1980 through 2004. Various driving laws are indicated in the data set, such as the alcohol level at which drivers are considered legally intoxicated. There are also indicators for “per se” laws—where licenses can be revoked without a trial—and seat belt laws. A few economics and demographic variables are also included. The description of the each of the variables in the dataset is come with the dataste.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#Load libraries
sh <- suppressPackageStartupMessages
sh(library(Hmisc));sh(library(plm));sh(library(ggplot2));sh(library(gridExtra));sh(library(dplyr));sh(library(fpp3))
```

**Exercises:**

1. (40%) Load the data. Provide a description of the basic structure of the dataset, as we have done throughout the semester. Conduct a very thorough EDA, which should include both graphical and tabular techniques, on the dataset, including both the dependent variable *totfatrte* and the potential explanatory variables. You need to write a detailed narrative of your observations of your EDA. *Reminder: giving an "output dump" (i.e. providing a bunch of graphs and tables without description and hoping your audience will interpret them) will receive a zero in this exercise.*
```{r}
load(file = "driving.RData")
driving <- data
sum(is.na(driving))
nrow(driving)
colnames(driving)
```

There are no missing values in the dataset and there are 1200 observations.
Here the subjects are indicated by the state variable. For each state there are multiple observations, one for each year.

```{r}
head(driving,10)
table(driving$state)
table(driving$year)
```
For each state there are 25 observations one for each year. For each year there are 48 observations, one for each state. *This panel is balanced*. There are a total of 1200 observations and since $1200 = 25X48$ we have a balanced panel.
d80 through d04 are the indicator variables for each of the time periods.

We cannot use OLS here because we suspect voilation of the independence assumptions. Let us confirm that using the Durbin-Watson test.

```{r}
library(lmtest)
dwtest(totfatrte ~ seatbelt + zerotol + slnone, data=driving)
```
Null hypothesis is rejected, this confirms the voilation of the independence assumption.

So we will have to use panel methods here. There are 25 panels in this dataset, one for each year of observations.

Let's see the response variable (total fatality rate) distribution broken down by panels

```{r}
ggplot(driving, aes(factor(year), totfatrte)) +  geom_boxplot(aes(fill = factor(year))) + geom_jitter() + ggtitle("Total fatality rate by year")
```

Overall, the total fatality rate has dropped over the years.

```{r}
ggplot(driving, aes(year, totfatrte)) +  geom_point(aes(color = factor(seatbelt)))
```

Early years, pre-1985 there seems to have been no seatbelt law. The fatality rate dropped over the years as 1, 2 seatbelt laws were introduced. States started with introduction of seatbelt rule 2 and then majority of them moved to seatbelt rule 1.

```{r}
ggplot(driving, aes(year, totfatrte)) +  geom_point(aes(color = zerotol))
```


Most of  the states moved to a zero tolerance policy over the years. In fact all the states seems to have introduced theh zero tolerance policy. 

```{r}
ggplot(driving, aes(year, totfatrte)) +  geom_point(aes(color = sl65))
```


Most of the states seemed to have moved to speed limit 65 policy which correlates storngly with the drop in fatality rate. However several states seemed to have moved away to a possibly lower limit (55 mph) post 1996.

```{r}
ggplot(driving, aes(year, log(statepop))) +  geom_point(aes(color = factor(state)))
```

**TODO: find the variables that are time invariant and the ones which are time varying. Perform specific EDA on those varibles.**


We could take a look at the panel data after inserting a structure into the dataset. The indices here are the "state" and the "year".  
We will have to select the appropriate explanatory variables to look at the correlation between those variables and the fatality rate. We would drop some of variables which are likely to have a higher correlation such as "fatality rate" with "weekend fatality rate" and "night fatality rate" etc. from the dataset used for the correlation matrix. 
```{r}
library(plm)
library(corrplot)
driving.panel <- pdata.frame(driving, c("state","year"), drop.index = TRUE)
M <- cor(driving.panel[,c(1:12,19:20,23:28,54)])
corrplot(M, method='circle')
```

We see that the "perc_14_24" which is the percent population between 14 and 24 has a very strong correlation with the "fatality rate". The 'minimum age' has a negative correlation with the fatality rate, so is the 'zero tolerance' even though it is not very strong.

2. (15%) How is the our dependent variable of interest *totfatrte* defined? What is the average of this variable in each of the years in the time period covered in this dataset? Estimate a linear regression model of *totfatrte* on a set of dummy variables for the years 1981 through 2004. What does this model explain? Describe what you find in this model. Did driving become safer over this period? Please provide a detailed explanation.

Running OLS only on data for one panel only, the equation can be written as below

*Fixed year effect* 

$$totfatrte_{it} = \beta_0 + \delta_{81} d81 + \delta_{82} d82 + ... + \delta_{04}d04 + u_{it}$$

$\delta_t$ the change that is common to every city in year $t$. It estimates the common change in the fatality rate in year $t$ relative to the base / reference year $1980$.
We assume that this change is common across all cities for a given year $t$.

```{r}
driving.ols <- lm(totfatrte ~ d81 + d82 + d83 + d84 + d85 + d86 + d87 + d88 + d89 + d90 + d91 + d92 + d93 + d94 + d95 + d96 + d97 + d98 + d99 + d00 + d01 + d02 + d03 + d04, data=driving)
summary(driving.ols)
```
From the above coefficients, we can say that for the year 2000, the fatality rate has dropped by 8.66 units as compared to the year 1980. Except for the year 1981, the coefficients for all the year dummies are significant. From this we can also say that the fatality rate has been dropping over the years. Driving did seem to become safer over the years.


3. (15%) Expand your model in *Exercise 2* by adding variables *bac08, bac10, perse, sbprim, sbsecon, sl70plus, gdl, perc14_24, unem, vehicmilespc*, and perhaps *transformations of some or all of these variables*. Please explain carefully your rationale, which should be based on your EDA, behind any transformation you made. If no transformation is made, explain why transformation is not needed. How are the variables *bac8* and *bac10* defined? Interpret the coefficients on *bac8* and *bac10*. Do *per se laws* have a negative effect on the fatality rate? What about having a primary seat belt law? (Note that if a law was enacted sometime within a year the fraction of the year is recorded in place of the zero-one indicator.)

4. (15%) Reestimate the model from *Exercise 3* using a fixed effects (at the state level) model. How do the coefficients on *bac08, bac10, perse, and sbprim* compare with the pooled OLS estimates? Which set of estimates do you think is more reliable? What assumptions are needed in each of these models?  Are these assumptions reasonable in the current context?

5. (5%) Would you perfer to use a random effects model instead of the fixed effects model you built in *Exercise 4*? Please explain.

6. (5%) Suppose that *vehicmilespc*, the number of miles driven per capita, increases by $1,000$. Using the FE estimates, what is the estimated effect on *totfatrte*? Please interpret the estimate.

7. (5%) If there is serial correlation or heteroskedasticity in the idiosyncratic errors of the model, what would be the consequences on the estimators and their standard errors?













